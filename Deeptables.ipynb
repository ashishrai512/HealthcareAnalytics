{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.metrics import accuracy_score,roc_auc_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.impute import SimpleImputer\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('train.csv')\n",
    "test=pd.read_csv('test.csv')\n",
    "\n",
    "\n",
    "dict_stay={'0-10':0,'11-20':1, '21-30':2, '31-40':3, '41-50':4, '51-60':5, '61-70':6, '71-80':7,\n",
    "        '81-90':8, '91-100':9, 'More than 100 Days':10}\n",
    "\n",
    "train['Stay'].replace(dict_stay,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X=train.drop(['Stay'],axis=1)\n",
    "\n",
    "train_X['type']='train'\n",
    "test['type']='test'\n",
    "data=pd.concat([train_X,test])\n",
    "\n",
    "#fill na using group by\n",
    "data['Bed Grade']=data.groupby(['Hospital_code'])['Bed Grade'].\\\n",
    "transform(lambda x: x.fillna(x.mode()[0] if not x.mode().empty else np.nan))\n",
    "\n",
    "data['City_Code_Patient']=data.groupby(['Hospital_region_code'])['City_Code_Patient'].\\\n",
    "transform(lambda x: x.fillna(x.mode()[0] if not x.mode().empty else np.nan))\n",
    "\n",
    "cat_col=['Hospital_code', 'Hospital_type_code', 'City_Code_Hospital',\n",
    "       'Hospital_region_code','Department', 'Ward_Type', 'Ward_Facility_Code',\n",
    "       'City_Code_Patient', 'Type of Admission',\n",
    "       'Severity of Illness','Age']\n",
    "\n",
    "float_col=['Available Extra Rooms in Hospital','Visitors with Patient','Admission_Deposit','Bed Grade']\n",
    "\n",
    "for col in cat_col:\n",
    "    data[col]=le.fit_transform(data[col])\n",
    "\n",
    "for col in float_col:\n",
    "    data[col]=data[col].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X=data[data['type']=='train']\n",
    "test_=data[data['type']=='test']\n",
    "train_X.drop(['type'],axis=1,inplace=True)\n",
    "test_.drop(['type'],axis=1,inplace=True)\n",
    "\n",
    "X = train_X.drop(['case_id','patientid'],axis=1)\n",
    "y = train['Stay']\n",
    "test_=test_[X.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeptables.models.deeptable import DeepTable, ModelConfig\n",
    "from deeptables.datasets import dsutils\n",
    "from deeptables.models import deeptable\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler , EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():  \n",
    "    conf = ModelConfig(dnn_params={'hidden_units':((300, 0.3, True),(300, 0.3, True),),\n",
    "                                'dnn_activation':'relu',},\n",
    "                            fixed_embedding_dim=True,\n",
    "                            embeddings_output_dim=20,\n",
    "                            nets =['pnn_nets'],\n",
    "                            stacking_op = 'add',\n",
    "                            output_use_bias = False,\n",
    "                            metrics=['accuracy'],\n",
    "                            categorical_columns = cat_col\n",
    "                        )\n",
    "    dt = DeepTable(config = conf)\n",
    "    \n",
    "    return dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100 \n",
    "batch_size = 256\n",
    "seeds = [32,432 ,73]\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience = 5 ,restore_best_weights= True)\n",
    "annealer = LearningRateScheduler(lambda x: 1e-3 * 0.95 ** x) #for plateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      " rows of train = 175140 rows of holdout = 143298\n",
      "11 class detected, inferred as a [multiclass classification] task\n",
      "Preparing features cost:0.019106149673461914\n",
      "Imputation cost:0.08170890808105469\n",
      "Categorical encoding cost:0.16249394416809082\n",
      "fit_transform cost:0.29991698265075684\n",
      "transform_X cost:9.318440198898315\n",
      "transform_y cost:0.0046389102935791016\n",
      ">>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (11)', 'input_continuous_all: (4)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [34, 9, 13, 5, 7, 8, 8, 39, 5, 5, 12]\n",
      "output_dims: [20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20]\n",
      "dropout: 0.3\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 224)\n",
      "---------------------------------------------------------\n",
      "nets: ['pnn_nets']\n",
      "---------------------------------------------------------\n",
      "pnn-inner_product: input_shape list(11), output_shape (None, 55)\n",
      "pnn-outer_product: input_shape list(11), output_shape (None, 55)\n",
      "pnn-dnn: input_shape (None, 334), output_shape (None, 300)\n",
      "---------------------------------------------------------\n",
      "stacking_op: add\n",
      "---------------------------------------------------------\n",
      "output: activation: softmax, output_shape: (None, 11), use_bias: False\n",
      "loss: categorical_crossentropy\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "Model has been saved to:dt_output/dt_20200907 121707_pnn_nets/pnn_nets.h5\n",
      "transform_X cost:11.017089128494263\n",
      "transform_y cost:0.0045588016510009766\n",
      "LGB Val CV= 42.84358322620392\n",
      "####################################################################################################\n",
      "\n",
      "\n",
      "Fold 1\n",
      " rows of train = 175140 rows of holdout = 143298\n",
      "11 class detected, inferred as a [multiclass classification] task\n",
      "Preparing features cost:0.0222780704498291\n",
      "Imputation cost:0.1266312599182129\n",
      "Categorical encoding cost:0.24489307403564453\n",
      "fit_transform cost:0.4379100799560547\n",
      "transform_X cost:10.321346998214722\n",
      "transform_y cost:0.006649971008300781\n",
      ">>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (11)', 'input_continuous_all: (4)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [34, 9, 13, 5, 7, 8, 8, 39, 5, 5, 12]\n",
      "output_dims: [20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20]\n",
      "dropout: 0.3\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 224)\n",
      "---------------------------------------------------------\n",
      "nets: ['pnn_nets']\n",
      "---------------------------------------------------------\n",
      "pnn-inner_product: input_shape list(11), output_shape (None, 55)\n",
      "pnn-outer_product: input_shape list(11), output_shape (None, 55)\n",
      "pnn-dnn: input_shape (None, 334), output_shape (None, 300)\n",
      "---------------------------------------------------------\n",
      "stacking_op: add\n",
      "---------------------------------------------------------\n",
      "output: activation: softmax, output_shape: (None, 11), use_bias: False\n",
      "loss: categorical_crossentropy\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "Model has been saved to:dt_output/dt_20200907 124821_pnn_nets/pnn_nets.h5\n",
      "transform_X cost:10.075222730636597\n",
      "transform_y cost:0.004928112030029297\n",
      "LGB Val CV= 42.808693647384644\n",
      "####################################################################################################\n",
      "\n",
      "\n",
      "Fold 2\n",
      " rows of train = 175140 rows of holdout = 143298\n",
      "11 class detected, inferred as a [multiclass classification] task\n",
      "Preparing features cost:0.018168926239013672\n",
      "Imputation cost:0.08498692512512207\n",
      "Categorical encoding cost:0.1573030948638916\n",
      "fit_transform cost:0.29677486419677734\n",
      "transform_X cost:9.396305799484253\n",
      "transform_y cost:0.0045511722564697266\n",
      ">>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (11)', 'input_continuous_all: (4)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [34, 9, 13, 5, 7, 8, 8, 39, 5, 5, 12]\n",
      "output_dims: [20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20]\n",
      "dropout: 0.3\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 224)\n",
      "---------------------------------------------------------\n",
      "nets: ['pnn_nets']\n",
      "---------------------------------------------------------\n",
      "pnn-inner_product: input_shape list(11), output_shape (None, 55)\n",
      "pnn-outer_product: input_shape list(11), output_shape (None, 55)\n",
      "pnn-dnn: input_shape (None, 334), output_shape (None, 300)\n",
      "---------------------------------------------------------\n",
      "stacking_op: add\n",
      "---------------------------------------------------------\n",
      "output: activation: softmax, output_shape: (None, 11), use_bias: False\n",
      "loss: categorical_crossentropy\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "Model has been saved to:dt_output/dt_20200907 132750_pnn_nets/pnn_nets.h5\n",
      "transform_X cost:11.760944128036499\n",
      "transform_y cost:0.006337165832519531\n",
      "LGB Val CV= 43.00757944583893\n",
      "####################################################################################################\n",
      "\n",
      "\n",
      "Fold 3\n",
      " rows of train = 175140 rows of holdout = 143298\n",
      "11 class detected, inferred as a [multiclass classification] task\n",
      "Preparing features cost:0.02010798454284668\n",
      "Imputation cost:0.08383893966674805\n",
      "Categorical encoding cost:0.16509318351745605\n",
      "fit_transform cost:0.30642271041870117\n",
      "transform_X cost:11.368104934692383\n",
      "transform_y cost:0.005159854888916016\n",
      ">>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (11)', 'input_continuous_all: (4)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [34, 9, 13, 5, 7, 8, 8, 39, 5, 5, 12]\n",
      "output_dims: [20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20]\n",
      "dropout: 0.3\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 224)\n",
      "---------------------------------------------------------\n",
      "nets: ['pnn_nets']\n",
      "---------------------------------------------------------\n",
      "pnn-inner_product: input_shape list(11), output_shape (None, 55)\n",
      "pnn-outer_product: input_shape list(11), output_shape (None, 55)\n",
      "pnn-dnn: input_shape (None, 334), output_shape (None, 300)\n",
      "---------------------------------------------------------\n",
      "stacking_op: add\n",
      "---------------------------------------------------------\n",
      "output: activation: softmax, output_shape: (None, 11), use_bias: False\n",
      "loss: categorical_crossentropy\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "avg_loss = []\n",
    "\n",
    "X_train_cv,y_train_cv = X.copy(), y.copy()\n",
    "\n",
    "sssf = StratifiedShuffleSplit(n_splits=5, test_size = 0.45 ,random_state=1)\n",
    "\n",
    "for i, (idxT, idxV) in enumerate(sssf.split(X_train_cv, y_train_cv)):  \n",
    "    \n",
    "    steps_per_epoch = len(X_train_cv.iloc[idxT])//batch_size  \n",
    "    validation_steps = len(X_train_cv.iloc[idxV])//batch_size\n",
    "    \n",
    "    print('Fold',i)\n",
    "    print(' rows of train =',len(idxT),'rows of holdout =',len(idxV))\n",
    "    \n",
    "    dt_cv =  build_model()\n",
    "    model_dnn_cv, history_cv = dt_cv.fit(X_train_cv.iloc[idxT], y_train_cv.iloc[idxT],\n",
    "                                                 validation_data = (X_train_cv.iloc[idxV],y_train_cv.iloc[idxV]),\n",
    "                                                 steps_per_epoch = steps_per_epoch,\n",
    "                                                 validation_steps = validation_steps,\n",
    "                                                 batch_size=batch_size, epochs=epochs, \n",
    "                                                 verbose=0, callbacks=[early_stop,annealer])\n",
    "    \n",
    "    val_stats = dt_cv.evaluate(X_train_cv.iloc[idxV],y_train_cv.iloc[idxV], batch_size=batch_size, verbose=0)\n",
    "    \n",
    "    acc= val_stats['accuracy']*100\n",
    "    scores.append(acc)      \n",
    "    avg_loss.append(val_stats['loss'])\n",
    "    print ('LGB Val CV=',acc)\n",
    "    print('#'*100)\n",
    "    print('\\n')\n",
    "    \n",
    "print(\"Multi Log Loss Stats {0:.5f},{1:.5f}\".format(np.array(avg_loss).mean(), np.array(avg_loss).std()))\n",
    "print('%.3f (%.3f)' % (np.array(scores).mean(), np.array(scores).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
